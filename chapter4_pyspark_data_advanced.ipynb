{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cbd99b7",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 4: Advanced Data Engineering Techniques\n",
    "\n",
    "Welcome to Chapter 4! In this notebook, you'll practice advanced PySpark data engineering techniques, including UDFs, Spark SQL, partitioning, working with nested data, and more. This chapter is standalone: all data is freshly loaded and cleaned before you begin practicing.\n",
    "\n",
    "## What You'll Do\n",
    "- Practice advanced data engineering techniques on real-world data\n",
    "- Learn with generic sample syntax, then apply concepts to your actual DataFrames\n",
    "- Tackle interview-style and real-world analytics questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef1dffe",
   "metadata": {},
   "source": [
    "\n",
    "## Important Instructions\n",
    "- Sample syntax is for illustration only and uses generic DataFrame names (e.g., `df`, `input_df`).\n",
    "- Always use the actual DataFrame names provided in the practice questions (e.g., `customers_df`, `orders_df`).\n",
    "- Do not copy-paste the sample code for the practice question. Try to solve it yourself using the actual DataFrame.\n",
    "- This is for your own practice, so type the commands even if the question is similar to the example.\n",
    "- Don't execute the code mentioned in syntax as it may modify the data.\n",
    "- Avoid using AI for code completion.\n",
    "- Play around and try out a few more for your understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbdb6e9",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preparation (Run This First)\n",
    "This section downloads, loads, and cleans all datasets so you can start advanced practice without running previous chapters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e3838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the data\n",
    "!wget -O customers.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/customers.csv\n",
    "!wget -O products.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/products.csv\n",
    "!wget -O orders.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/orders.csv\n",
    "!wget -O order_items.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/order_items.csv\n",
    "!wget -O employees.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/employees.csv\n",
    "!wget -O transactions.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/transactions.csv\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySpark Advanced Practice\").getOrCreate()\n",
    "\n",
    "# Load DataFrames\n",
    "df_map = {}\n",
    "for name in [\"customers\", \"products\", \"orders\", \"order_items\", \"employees\", \"transactions\"]:\n",
    "    df_map[name] = spark.read.csv(f\"{name}.csv\", header=True, inferSchema=True)\n",
    "\n",
    "customers_df = df_map[\"customers\"]\n",
    "products_df = df_map[\"products\"]\n",
    "orders_df = df_map[\"orders\"]\n",
    "order_items_df = df_map[\"order_items\"]\n",
    "employees_df = df_map[\"employees\"]\n",
    "transactions_df = df_map[\"transactions\"]\n",
    "\n",
    "# Data cleaning (minimal, for advanced topics):\n",
    "customers_df = customers_df.dropDuplicates().dropna(subset=[\"customer_id\", \"name\", \"email\"])\n",
    "products_df = products_df.dropDuplicates().dropna(subset=[\"product_id\", \"product_name\", \"price\"])\n",
    "orders_df = orders_df.dropDuplicates().dropna(subset=[\"order_id\", \"customer_id\", \"order_amount\"])\n",
    "order_items_df = order_items_df.dropDuplicates().dropna(subset=[\"order_item_id\", \"order_id\", \"product_id\"])\n",
    "employees_df = employees_df.dropDuplicates().dropna(subset=[\"employee_id\", \"name\"])\n",
    "transactions_df = transactions_df.dropDuplicates().dropna(subset=[\"transaction_id\", \"customer_id\", \"amount\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d566c77",
   "metadata": {},
   "source": [
    "\n",
    "## Table Overview\n",
    "- **customers_df**: customer_id, name, email, phone, address, registration_date, status\n",
    "- **products_df**: product_id, product_name, category, price, stock_quantity\n",
    "- **orders_df**: order_id, customer_id, order_date, order_amount, order_status, payment_method\n",
    "- **order_items_df**: order_item_id, order_id, product_id, quantity, item_total\n",
    "- **employees_df**: employee_id, name, department, hire_date, salary, manager_id\n",
    "- **transactions_df**: transaction_id, customer_id, transaction_date, amount, transaction_type, location, created_at\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42f828c",
   "metadata": {},
   "source": [
    "\n",
    "### 1. User-Defined Functions (UDFs) & Built-in Functions\n",
    "\n",
    "**Concept:** UDFs allow you to define custom logic for transforming data. Use built-in functions whenever possible for better performance.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def my_func(x):\n",
    "    return x.upper() if x else None\n",
    "\n",
    "my_udf = udf(my_func, StringType())\n",
    "df.withColumn(\"new_col\", my_udf(df.col1))\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Create a UDF to mask email addresses in `customers_df`.\n",
    "- Use a built-in function to convert all product names to uppercase in `products_df`.\n",
    "\n",
    "**Expected Output:**\n",
    "- DataFrames with transformed columns.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Create a UDF to categorize transactions as \"High\" or \"Low\" based on amount in `transactions_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c5b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd145f",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Spark SQL\n",
    "\n",
    "**Concept:** Register DataFrames as temporary views and use SQL queries for analysis.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "spark.sql(\"SELECT col1, COUNT(*) FROM my_table GROUP BY col1\")\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Register `orders_df` as a temp view and write a SQL query to find the total order amount per customer.\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with customer_id and total order amount.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Write a SQL query to find the top 3 products by total quantity sold in `order_items_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d08e75",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Data Partitioning, Bucketing, and Caching\n",
    "\n",
    "**Concept:** Partitioning and bucketing optimize data storage and query performance. Caching stores DataFrames in memory for faster access.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "# Repartition\n",
    "df = df.repartition(4, \"col1\")\n",
    "# Cache\n",
    "df.cache()\n",
    "# Bucketing (when writing to disk)\n",
    "df.write.bucketBy(4, \"col1\").saveAsTable(\"bucketed_table\")\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Repartition `orders_df` by `customer_id`.\n",
    "- Cache `products_df` for repeated use.\n",
    "\n",
    "**Expected Output:**\n",
    "- DataFrames partitioned/cached as specified.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Write `order_items_df` to disk using bucketing by `product_id` (if supported in your environment).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b35f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1dd519",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Data Writing and Saving\n",
    "\n",
    "**Concept:** Save DataFrames to disk in various formats and modes.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "df.write.csv(\"output.csv\", mode=\"overwrite\")\n",
    "df.write.parquet(\"output.parquet\", partitionBy=[\"col1\"])\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Save `customers_df` as a Parquet file partitioned by `status`.\n",
    "- Save `orders_df` as a CSV file.\n",
    "\n",
    "**Expected Output:**\n",
    "- Files written to disk in the specified format.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Save `transactions_df` as a JSON file, partitioned by `transaction_type`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dc4586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aedcb4e",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Error Handling and Data Quality Checks\n",
    "\n",
    "**Concept:** Handle corrupt records, schema mismatches, and validate data quality.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "# Handle corrupt records when reading\n",
    "df = spark.read.option(\"mode\", \"DROPMALFORMED\").csv(\"file.csv\")\n",
    "# Data validation\n",
    "from pyspark.sql.functions import col\n",
    "invalid_rows = df.filter(col(\"col1\").isNull())\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Find and count rows in `orders_df` with null `order_amount`.\n",
    "- Drop rows in `products_df` with negative price.\n",
    "\n",
    "**Expected Output:**\n",
    "- DataFrames with invalid rows identified or removed.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Write a UDF to flag invalid email addresses in `customers_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e775680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b96d97e",
   "metadata": {},
   "source": [
    "\n",
    "### 6. Exploding and Flattening Nested Data\n",
    "\n",
    "**Concept:** Use `explode` and `posexplode` to flatten arrays and structs in DataFrames.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "from pyspark.sql.functions import explode\n",
    "# Assume df has a column 'arr_col' which is an array\n",
    "df_exploded = df.withColumn(\"element\", explode(df.arr_col))\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Create a DataFrame with an array column and use `explode` to flatten it.\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with one row per array element.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Use `posexplode` to get both position and value from an array column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8892d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5115c7",
   "metadata": {},
   "source": [
    "\n",
    "### 7. Working with Complex/Nested Data Types\n",
    "\n",
    "**Concept:** Handle arrays, structs, and maps in DataFrames. Access and manipulate nested fields.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "# Access a field in a struct\n",
    "df.select(col(\"struct_col.field1\"))\n",
    "# Access an element in an array\n",
    "df.select(col(\"array_col\")[0])\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Create a DataFrame with a struct column and select nested fields.\n",
    "- Create a DataFrame with a map column and access values by key.\n",
    "\n",
    "**Expected Output:**\n",
    "- DataFrames with selected nested fields or map values.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Flatten a DataFrame with multiple levels of nested structs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8439bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1173c51",
   "metadata": {},
   "source": [
    "\n",
    "### 8. Date/Time and Timezone Handling\n",
    "\n",
    "**Concept:** Advanced date/time manipulations, working with time zones, and timestamp conversions.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "from pyspark.sql.functions import to_timestamp, from_utc_timestamp\n",
    "# Convert string to timestamp\n",
    "df = df.withColumn(\"ts\", to_timestamp(df.col1, \"yyyy-MM-dd HH:mm:ss\"))\n",
    "# Convert UTC to local timezone\n",
    "df = df.withColumn(\"local_ts\", from_utc_timestamp(df.ts, \"Asia/Kolkata\"))\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Convert `created_at` in `transactions_df` to a timestamp and extract the hour.\n",
    "- Convert UTC timestamps to IST in any DataFrame.\n",
    "\n",
    "**Expected Output:**\n",
    "- DataFrames with new timestamp or timezone-adjusted columns.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Calculate the time difference in hours between two timestamp columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb7158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
