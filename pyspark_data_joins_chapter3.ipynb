{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323cc08c",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 3: Joining DataFrames & Data Integration\n",
    "\n",
    "Welcome to Chapter 3! In this notebook, you'll master all types of joins and data integration techniques in PySpark using realistic datasets. This chapter is standalone: all data is freshly loaded and cleaned before you begin practicing.\n",
    "\n",
    "## What You'll Do\n",
    "- Practice all major join types and integration scenarios on real-world data\n",
    "- Learn with generic sample syntax, then apply concepts to your actual DataFrames\n",
    "- Tackle interview-style and real-world analytics questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490bf055",
   "metadata": {},
   "source": [
    "\n",
    "## Important Instructions\n",
    "- Sample syntax is for illustration only and uses generic DataFrame names (e.g., `df1`, `df2`, `input_df`).\n",
    "- Always use the actual DataFrame names provided in the practice questions (e.g., `customers_df`, `orders_df`).\n",
    "- Do not copy-paste the sample code for the practice question. Try to solve it yourself using the actual DataFrame.\n",
    "- This is for your own practice, so type the commands even if the question is similar to the example.\n",
    "- Don't execute the code mentioned in syntax as it may modify the data.\n",
    "- Avoid using AI for code completion.\n",
    "- Play around and try out a few more for your understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93431989",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preparation (Run This First)\n",
    "This section downloads, loads, and cleans all datasets so you can start joining and integrating data without running previous chapters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939f6698",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the data\n",
    "!wget -O customers.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/customers.csv\n",
    "!wget -O products.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/products.csv\n",
    "!wget -O orders.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/orders.csv\n",
    "!wget -O order_items.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/order_items.csv\n",
    "!wget -O employees.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/employees.csv\n",
    "!wget -O transactions.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/transactions.csv\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySpark Join Practice\").getOrCreate()\n",
    "\n",
    "# Load DataFrames\n",
    "df_map = {}\n",
    "for name in [\"customers\", \"products\", \"orders\", \"order_items\", \"employees\", \"transactions\"]:\n",
    "    df_map[name] = spark.read.csv(f\"{name}.csv\", header=True, inferSchema=True)\n",
    "\n",
    "customers_df = df_map[\"customers\"]\n",
    "products_df = df_map[\"products\"]\n",
    "orders_df = df_map[\"orders\"]\n",
    "order_items_df = df_map[\"order_items\"]\n",
    "employees_df = df_map[\"employees\"]\n",
    "transactions_df = df_map[\"transactions\"]\n",
    "\n",
    "# Data cleaning (minimal, for joins):\n",
    "customers_df = customers_df.dropDuplicates().dropna(subset=[\"customer_id\", \"name\", \"email\"])\n",
    "products_df = products_df.dropDuplicates().dropna(subset=[\"product_id\", \"product_name\", \"price\"])\n",
    "orders_df = orders_df.dropDuplicates().dropna(subset=[\"order_id\", \"customer_id\", \"order_amount\"])\n",
    "order_items_df = order_items_df.dropDuplicates().dropna(subset=[\"order_item_id\", \"order_id\", \"product_id\"])\n",
    "employees_df = employees_df.dropDuplicates().dropna(subset=[\"employee_id\", \"name\"])\n",
    "transactions_df = transactions_df.dropDuplicates().dropna(subset=[\"transaction_id\", \"customer_id\", \"amount\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d21d45f",
   "metadata": {},
   "source": [
    "\n",
    "## Table Overview\n",
    "- **customers_df**: customer_id, name, email, phone, address, registration_date, status\n",
    "- **products_df**: product_id, product_name, category, price, stock_quantity\n",
    "- **orders_df**: order_id, customer_id, order_date, order_amount, order_status, payment_method\n",
    "- **order_items_df**: order_item_id, order_id, product_id, quantity, item_total\n",
    "- **employees_df**: employee_id, name, department, hire_date, salary, manager_id\n",
    "- **transactions_df**: transaction_id, customer_id, transaction_date, amount, transaction_type, location, created_at\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e51922b",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Introduction to Joins\n",
    "\n",
    "**Concept:** Joins combine rows from two or more DataFrames based on a related column. Types include inner, left, right, full, semi, and anti joins.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "df1.join(df2, df1.col1 == df2.col2, \"inner\")\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- List all join types available in PySpark and briefly describe each.\n",
    "\n",
    "**Expected Output:**\n",
    "- A markdown/text cell with join type descriptions.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Give a real-world example for each join type using your data tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69309aa4",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Basic Join Syntax\n",
    "\n",
    "**Concept:** Use the `join()` method to combine DataFrames. Specify join columns and type.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "joined_df = df1.join(df2, df1.col1 == df2.col2, \"inner\")\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Join `orders_df` with `customers_df` to get customer details for each order.\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with order and customer columns.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Join `order_items_df` with `products_df` to get product details for each order item.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f725109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba1960",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Inner Join\n",
    "\n",
    "**Concept:** Returns rows with matching keys in both DataFrames.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "result_df = df1.join(df2, df1.key == df2.key, \"inner\")\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Get all orders with valid customers (inner join `orders_df` and `customers_df`).\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with only orders that have a matching customer.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Get all order items with valid products (inner join `order_items_df` and `products_df`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64731eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af38ee",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Left, Right, and Full Outer Joins\n",
    "\n",
    "**Concept:**\n",
    "- Left: All rows from left, matched rows from right\n",
    "- Right: All rows from right, matched rows from left\n",
    "- Full: All rows from both, matched where possible\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "left_df = df1.join(df2, df1.key == df2.key, \"left\")\n",
    "right_df = df1.join(df2, df1.key == df2.key, \"right\")\n",
    "full_df = df1.join(df2, df1.key == df2.key, \"outer\")\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Find all customers and their orders (left join `customers_df` and `orders_df`).\n",
    "- Find all products and their order items (right join `order_items_df` and `products_df`).\n",
    "\n",
    "**Expected Output:**\n",
    "- DataFrames showing all left/right/full join results.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Find all orders and their order items (full outer join `orders_df` and `order_items_df`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfb1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8315a5f3",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Semi and Anti Joins\n",
    "\n",
    "**Concept:**\n",
    "- Semi: Returns rows from left where a match exists in right (no columns from right)\n",
    "- Anti: Returns rows from left where no match exists in right\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "semi_df = df1.join(df2, df1.key == df2.key, \"left_semi\")\n",
    "anti_df = df1.join(df2, df1.key == df2.key, \"left_anti\")\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Find customers who have placed at least one order (semi join `customers_df` and `orders_df`).\n",
    "- Find customers who have never placed an order (anti join `customers_df` and `orders_df`).\n",
    "\n",
    "**Expected Output:**\n",
    "- DataFrames with only the relevant customers.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Find products that have never been ordered (anti join `products_df` and `order_items_df`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c3767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9acd0",
   "metadata": {},
   "source": [
    "\n",
    "### 6. Joining Multiple DataFrames\n",
    "\n",
    "**Concept:** Chain multiple joins to combine more than two DataFrames.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "joined_df = df1.join(df2, ...).join(df3, ...)\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Get order details with product names and customer info (join `orders_df`, `order_items_df`, `products_df`, and `customers_df`).\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with order, product, and customer columns.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- For each transaction, get the customer name and all related orders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd18379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c452ab66",
   "metadata": {},
   "source": [
    "\n",
    "### 7. Handling Duplicate Columns and Name Conflicts\n",
    "\n",
    "**Concept:** Use `alias`, `select`, and `drop` to resolve duplicate column names after joins.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "df1 = df1.alias(\"a\")\n",
    "df2 = df2.alias(\"b\")\n",
    "joined_df = df1.join(df2, df1.key == df2.key)\n",
    "# Use select to pick/rename columns\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Join `orders_df` and `customers_df`, then select only one `customer_id` column and rename as needed.\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with no duplicate columns.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Join `order_items_df` and `products_df`, then select and rename columns for reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39befc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62bbf8b",
   "metadata": {},
   "source": [
    "\n",
    "### 8. Join Conditions Beyond Equality\n",
    "\n",
    "**Concept:** Joins can use expressions, not just equality (e.g., date ranges).\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "from pyspark.sql.functions import expr\n",
    "joined_df = df1.join(df2, expr(\"df1.col1 >= df2.col2\"), \"inner\")\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Join `transactions_df` to `orders_df` by customer_id and where transaction_date is within 7 days of order_date.\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with joined transactions and orders.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Join `employees_df` to itself to get each employee and their manager's name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f437ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c66684",
   "metadata": {},
   "source": [
    "\n",
    "### 9. Broadcast Joins\n",
    "\n",
    "**Concept:** Use `broadcast()` to optimize joins with small DataFrames.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "joined_df = df1.join(broadcast(df2), df1.key == df2.key)\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Broadcast join `products_df` (if small) to `order_items_df` for performance.\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame joined efficiently.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Broadcast join a small lookup table (e.g., department info) to `employees_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d8436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1981a01",
   "metadata": {},
   "source": [
    "\n",
    "### 10. Dealing with Nulls and Missing Keys\n",
    "\n",
    "**Concept:** Nulls and missing keys can affect join results. Handle them with care.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "# Filter out null keys before join\n",
    "df1 = df1.filter(df1.key.isNotNull())\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Find orders with missing customer references (left join and filter where customer columns are null).\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with orders that have no matching customer.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Find order items with missing product references.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6998e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b1b21",
   "metadata": {},
   "source": [
    "\n",
    "### 11. Self Joins\n",
    "\n",
    "**Concept:** Join a DataFrame to itself to relate rows (e.g., employees and managers).\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "df1 = df.alias(\"a\")\n",
    "df2 = df.alias(\"b\")\n",
    "joined_df = df1.join(df2, df1.manager_id == df2.employee_id)\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Join `employees_df` to itself to get each employee and their manager's name.\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with employee and manager columns.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Find employees who are not managers of anyone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38874bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87c477e",
   "metadata": {},
   "source": [
    "\n",
    "### 12. Real-World Scenarios & Challenges\n",
    "\n",
    "- Many-to-many joins: Customers and products via orders and order_items\n",
    "- Joining with aggregated data: Customer with their total spend\n",
    "- Data enrichment: Adding external reference data (e.g., product categories)\n",
    "\n",
    "**Try to solve these using the concepts above!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a17b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
