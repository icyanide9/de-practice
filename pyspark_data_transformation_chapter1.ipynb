{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215a26d2",
   "metadata": {},
   "source": [
    "\n",
    "# PySpark Data Engineering Practice: End-to-End Hands-On\n",
    "\n",
    "Welcome to this hands-on PySpark data engineering practice series! This notebook (and the series) is designed to help you master real-world data engineering skills using PySpark, with a focus on practical, interview-style scenarios.\n",
    "\n",
    "## What You'll Do\n",
    "\n",
    "- Work with realistic, joinable datasets (Customers, Orders, Products, Order Items, Employees, Transactions) containing intentional data quality issues.\n",
    "- Practice a wide range of data engineering concepts, from data cleaning and transformation to advanced aggregations, joins, UDFs, Spark SQL, and performance optimization.\n",
    "- Learn by doing: Each topic includes explanations, sample code, practice questions, expected outputs, and additional challenges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c6d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the data (replace URLs with your actual GitHub repo links)\n",
    "!wget https://raw.githubusercontent.com/yourusername/yourrepo/main/customers.csv\n",
    "!wget https://raw.githubusercontent.com/yourusername/yourrepo/main/products.csv\n",
    "!wget https://raw.githubusercontent.com/yourusername/yourrepo/main/orders.csv\n",
    "!wget https://raw.githubusercontent.com/yourusername/yourrepo/main/order_items.csv\n",
    "!wget https://raw.githubusercontent.com/yourusername/yourrepo/main/employees.csv\n",
    "!wget https://raw.githubusercontent.com/yourusername/yourrepo/main/transactions.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5c1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install PySpark if not already installed\n",
    "!pip install pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder     .appName(\"PySpark Data Engineering Practice\")     .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4e39a9",
   "metadata": {},
   "source": [
    "\n",
    "## Table Overview\n",
    "\n",
    "**customers**\n",
    "Stores customer details: customer_id, name, email, phone, address, registration_date, status.\n",
    "*Issues included: missing values, duplicates, invalid formats, inconsistent phone numbers.*\n",
    "\n",
    "**products**\n",
    "Product catalog: product_id, product_name, category, price, stock_quantity.\n",
    "*Issues included: duplicates, missing values, zero/invalid values, inconsistent capitalization.*\n",
    "\n",
    "**orders**\n",
    "Customer orders: order_id, customer_id, order_date, order_amount, order_status, payment_method.\n",
    "*Issues included: duplicates, unknown/missing values, zero/invalid values, foreign key issues.*\n",
    "\n",
    "**order_items**\n",
    "Items in each order: order_item_id, order_id, product_id, quantity, item_total.\n",
    "*Issues included: duplicates, zero/invalid values, foreign key issues.*\n",
    "\n",
    "**employees**\n",
    "Employee details: employee_id, name, department, hire_date, salary, manager_id.\n",
    "*Issues included: duplicates, missing values, zero/invalid values, inconsistent department names.*\n",
    "\n",
    "**transactions**\n",
    "Customer transactions: transaction_id, customer_id, transaction_date, amount, transaction_type, location, created_at (timestamp).\n",
    "*Issues included: duplicates, missing/unknown/invalid values, foreign key issues, timestamp for time-based operations.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf22252",
   "metadata": {},
   "source": [
    "\n",
    "## Topics Covered in This Series\n",
    "\n",
    "This series is organized into five main chapters, each focusing on a core area of PySpark data engineering:\n",
    "\n",
    "1. **Data Transformation**\n",
    "   Use PySpark functions for cleaning, transforming, and enriching data.\n",
    "   *Topics: data cleaning, type conversion, string/date manipulation, filtering, new columns, renaming, sorting, sampling, etc.*\n",
    "\n",
    "2. **Data Aggregation**\n",
    "   Practice using `groupBy`, `agg`, `pivot`, and window functions to perform aggregations and derive insights.\n",
    "\n",
    "3. **Joining Data**\n",
    "   Master different types of joins (inner, outer, left, right) to combine data from multiple DataFrames.\n",
    "\n",
    "4. **User-Defined Functions (UDFs)**\n",
    "   Learn how to create and apply UDFs for custom data transformations.\n",
    "\n",
    "5. **Spark SQL**\n",
    "   Practice writing SQL queries against Spark DataFrames using `spark.sql`.\n",
    "\n",
    "6. **Performance Optimization**\n",
    "   Explore techniques like partitioning, caching, and broadcasting to improve query performance.\n",
    "\n",
    "---\n",
    "\n",
    "Each chapter will follow this structure for every concept:\n",
    "- **Concept Explanation**\n",
    "- **Sample Syntax**\n",
    "- **Practice Question**\n",
    "- **Expected Output**\n",
    "- **Additional Challenge**\n",
    "\n",
    "---\n",
    "\n",
    "**Letâ€™s get started!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf257c6a",
   "metadata": {},
   "source": [
    "# Chapter 1: Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab6185c",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Data Cleaning\n",
    "\n",
    "**Concept Explanation:**  \n",
    "Data cleaning is the process of identifying and correcting (or removing) errors and inconsistencies in data to improve its quality. Common tasks include handling missing values, removing duplicates, standardizing formats, and correcting typos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08851ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample Syntax\n",
    "# Remove duplicate customers\n",
    "customers_df = customers_df.dropDuplicates()\n",
    "\n",
    "# Drop rows with missing email\n",
    "customers_df = customers_df.dropna(subset=[\"email\"])\n",
    "\n",
    "# Fill missing phone numbers with 'Unknown'\n",
    "customers_df = customers_df.fillna({\"phone\": \"Unknown\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007301ee",
   "metadata": {},
   "source": [
    "\n",
    "**Practice Question:**  \n",
    "Clean the `customers` DataFrame by:\n",
    "- Removing duplicate records (based on all columns)\n",
    "- Dropping rows where the `name` or `email` is missing\n",
    "- Filling missing `phone` values with \"Unknown\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619272a2",
   "metadata": {},
   "source": [
    "\n",
    "**Expected Output:**  \n",
    "A cleaned DataFrame with no duplicate customers, no missing names or emails, and all phone numbers filled (no nulls).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda71b88",
   "metadata": {},
   "source": [
    "\n",
    "**Additional Challenge:**  \n",
    "Standardize all email addresses to lowercase and remove any leading/trailing spaces in the `name` column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8c658f",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data Type Conversion\n",
    "\n",
    "**Concept Explanation:**  \n",
    "Data type conversion ensures that each column in your DataFrame has the correct data type for analysis and processing. For example, converting a string date to a DateType, or a string number to IntegerType.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3344239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# Convert registration_date from string to date\n",
    "customers_df = customers_df.withColumn(\"registration_date\", to_date(col(\"registration_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Convert price from string to float\n",
    "products_df = products_df.withColumn(\"price\", col(\"price\").cast(\"float\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c2d99",
   "metadata": {},
   "source": [
    "\n",
    "**Practice Question:**  \n",
    "Convert the following columns to appropriate types:\n",
    "- `registration_date` in `customers` to DateType\n",
    "- `price` in `products` to FloatType\n",
    "- `order_amount` in `orders` to FloatType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370211be",
   "metadata": {},
   "source": [
    "\n",
    "**Expected Output:**  \n",
    "The specified columns should have the correct data types (date or float) in their respective DataFrames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45311e8",
   "metadata": {},
   "source": [
    "\n",
    "**Additional Challenge:**  \n",
    "Convert the `created_at` column in `transactions` to a TimestampType.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d23b219",
   "metadata": {},
   "source": [
    "\n",
    "## 3. String Manipulation\n",
    "\n",
    "**Concept Explanation:**  \n",
    "String manipulation involves extracting, replacing, or transforming text data. PySpark provides functions for trimming, changing case, extracting substrings, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef80c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import lower, trim, substring, split\n",
    "\n",
    "# Extract email domain\n",
    "customers_df = customers_df.withColumn(\"email_domain\", split(col(\"email\"), \"@\")[1])\n",
    "\n",
    "# Trim spaces from name\n",
    "customers_df = customers_df.withColumn(\"name\", trim(col(\"name\")))\n",
    "\n",
    "# Convert product_name to uppercase\n",
    "products_df = products_df.withColumn(\"product_name\", upper(col(\"product_name\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b7268a",
   "metadata": {},
   "source": [
    "\n",
    "**Practice Question:**  \n",
    "For the `customers` DataFrame:\n",
    "- Create a new column `email_domain` that contains only the domain part of the email.\n",
    "- Trim any leading/trailing spaces from the `name` column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5606b7",
   "metadata": {},
   "source": [
    "\n",
    "**Expected Output:**  \n",
    "A DataFrame with a new `email_domain` column and all names properly trimmed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa0ea6",
   "metadata": {},
   "source": [
    "\n",
    "**Additional Challenge:**  \n",
    "For the `products` DataFrame, create a new column `short_name` that contains the first 5 characters of the product name in uppercase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956501ed",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Filtering and Selection\n",
    "\n",
    "**Concept Explanation:**  \n",
    "Filtering and selection allow you to retrieve specific rows or columns based on conditions, such as selecting only active customers or orders above a certain amount.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f236528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select only active customers\n",
    "active_customers = customers_df.filter(col(\"status\") == \"Active\")\n",
    "\n",
    "# Select orders with amount greater than 100\n",
    "large_orders = orders_df.filter(col(\"order_amount\") > 100)\n",
    "\n",
    "# Select specific columns\n",
    "selected_customers = customers_df.select(\"customer_id\", \"name\", \"email\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af061ced",
   "metadata": {},
   "source": [
    "\n",
    "**Practice Question:**  \n",
    "From the `orders` DataFrame, select all orders with `order_amount` greater than 100 and `order_status` as \"Completed\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e488b4",
   "metadata": {},
   "source": [
    "\n",
    "**Expected Output:**  \n",
    "A DataFrame containing only completed orders with an amount greater than 100.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e48a7c5",
   "metadata": {},
   "source": [
    "\n",
    "**Additional Challenge:**  \n",
    "From the `customers` DataFrame, select all customers who registered after \"2022-01-01\" and have status \"Active\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3f3b15",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Creating New Columns\n",
    "\n",
    "**Concept Explanation:**  \n",
    "Creating new columns allows you to derive additional information from existing data, such as calculating tenure, categorizing values, or combining fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6da644",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import year, when\n",
    "\n",
    "# Calculate customer tenure in years\n",
    "customers_df = customers_df.withColumn(\"tenure_years\", 2025 - year(col(\"registration_date\")))\n",
    "\n",
    "# Categorize order amount\n",
    "orders_df = orders_df.withColumn(\n",
    "    \"order_size\",\n",
    "    when(col(\"order_amount\") > 200, \"Large\")\n",
    "    .when(col(\"order_amount\") > 50, \"Medium\")\n",
    "    .otherwise(\"Small\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82820fee",
   "metadata": {},
   "source": [
    "\n",
    "**Practice Question:**  \n",
    "Add a new column `tenure_years` to the `customers` DataFrame, representing the number of years since registration (assume current year is 2025).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ee9e4",
   "metadata": {},
   "source": [
    "\n",
    "**Expected Output:**  \n",
    "A DataFrame with a new `tenure_years` column showing the correct tenure for each customer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd6d1cd",
   "metadata": {},
   "source": [
    "\n",
    "**Additional Challenge:**  \n",
    "In the `orders` DataFrame, add a column `is_high_value` that is `True` if `order_amount` is greater than 250, otherwise `False`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce4eed4",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Renaming Columns\n",
    "\n",
    "**Concept Explanation:**  \n",
    "Renaming columns improves clarity and consistency in your DataFrames, especially when preparing data for downstream processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rename 'name' to 'customer_name'\n",
    "customers_df = customers_df.withColumnRenamed(\"name\", \"customer_name\")\n",
    "\n",
    "# Rename multiple columns\n",
    "products_df = products_df.withColumnRenamed(\"product_name\", \"name\").withColumnRenamed(\"stock_quantity\", \"stock\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6838113",
   "metadata": {},
   "source": [
    "\n",
    "**Practice Question:**  \n",
    "Rename the `name` column in the `customers` DataFrame to `customer_name`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347a86f",
   "metadata": {},
   "source": [
    "\n",
    "**Expected Output:**  \n",
    "A DataFrame where the column is now called `customer_name`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a10c3",
   "metadata": {},
   "source": [
    "\n",
    "**Additional Challenge:**  \n",
    "Rename the `order_amount` column in the `orders` DataFrame to `total_amount`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c356d59",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Sorting Data\n",
    "\n",
    "**Concept Explanation:**  \n",
    "Sorting arranges your data in a specific order based on one or more columns, which is useful for reporting, ranking, or preparing data for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52820c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort customers by registration_date descending\n",
    "customers_df = customers_df.orderBy(col(\"registration_date\").desc())\n",
    "\n",
    "# Sort products by price ascending\n",
    "products_df = products_df.orderBy(\"price\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb9a020",
   "metadata": {},
   "source": [
    "\n",
    "**Practice Question:**  \n",
    "Sort the `products` DataFrame by `price` in descending order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be3c0d5",
   "metadata": {},
   "source": [
    "\n",
    "**Expected Output:**  \n",
    "A DataFrame of products sorted from highest to lowest price.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c337f30e",
   "metadata": {},
   "source": [
    "\n",
    "**Additional Challenge:**  \n",
    "Sort the `customers` DataFrame by `tenure_years` (from the previous task) in ascending order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42482305",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Sampling Data\n",
    "\n",
    "**Concept Explanation:**  \n",
    "Sampling allows you to work with a subset of your data, which is useful for testing, prototyping, or when working with very large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f71a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Take a random 10% sample of customers\n",
    "sample_customers = customers_df.sample(fraction=0.1, seed=42)\n",
    "\n",
    "# Take a fixed number of rows\n",
    "sample_products = products_df.limit(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80959c",
   "metadata": {},
   "source": [
    "\n",
    "**Practice Question:**  \n",
    "Take a random sample of 5 customers from the `customers` DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45cd80f",
   "metadata": {},
   "source": [
    "\n",
    "**Expected Output:**  \n",
    "A DataFrame containing 5 randomly selected customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f551f9b",
   "metadata": {},
   "source": [
    "\n",
    "**Additional Challenge:**  \n",
    "Take a 20% random sample of the `orders` DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1679fb73",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Date Handling and Transformations\n",
    "\n",
    "**Concept Explanation:**  \n",
    "Date handling includes parsing strings to dates, extracting date parts, performing date arithmetic, formatting, and handling time zones or nulls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e6acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import to_date, year, month, datediff, current_date\n",
    "\n",
    "# Parse registration_date to date\n",
    "customers_df = customers_df.withColumn(\"registration_date\", to_date(col(\"registration_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Extract year and month\n",
    "customers_df = customers_df.withColumn(\"reg_year\", year(col(\"registration_date\")))\n",
    "customers_df = customers_df.withColumn(\"reg_month\", month(col(\"registration_date\")))\n",
    "\n",
    "# Calculate days since registration\n",
    "customers_df = customers_df.withColumn(\"days_since_registration\", datediff(current_date(), col(\"registration_date\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbff1cc",
   "metadata": {},
   "source": [
    "\n",
    "**Practice Question:**  \n",
    "For the `transactions` DataFrame, extract the year and month from the `created_at` timestamp into new columns `year` and `month`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ae8af",
   "metadata": {},
   "source": [
    "\n",
    "**Expected Output:**  \n",
    "A DataFrame with two new columns: `year` and `month`, showing the year and month of each transaction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaa1692",
   "metadata": {},
   "source": [
    "\n",
    "**Additional Challenge:**  \n",
    "Calculate the number of days between `transaction_date` and `created_at` for each transaction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163fc9af",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Date Range Filtering\n",
    "\n",
    "**Concept Explanation:**  \n",
    "Date range filtering allows you to select records that fall within a specific date or timestamp range, which is common in time-based analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efe0d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Filter orders between two dates\n",
    "start_date = '2023-01-15'\n",
    "end_date = '2023-02-10'\n",
    "filtered_orders = orders_df.filter((col('order_date') >= start_date) & (col('order_date') <= end_date))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f60d4ce",
   "metadata": {},
   "source": [
    "\n",
    "**Practice Question:**  \n",
    "Filter the `orders` DataFrame to include only orders placed between '2023-01-20' and '2023-02-05' (inclusive).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e3594",
   "metadata": {},
   "source": [
    "\n",
    "**Expected Output:**  \n",
    "A DataFrame containing only orders within the specified date range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce344a2",
   "metadata": {},
   "source": [
    "\n",
    "**Additional Challenge:**  \n",
    "Filter the `transactions` DataFrame to include only transactions that occurred in February 2023.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
