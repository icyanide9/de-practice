{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c2b543",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 2: Data Aggregation & Window Functions\n",
    "\n",
    "Welcome to Chapter 2! In this notebook, you'll master PySpark's aggregation and windowing capabilities using realistic datasets. This chapter is standalone: all data is freshly loaded and cleaned before you begin practicing.\n",
    "\n",
    "## What You'll Do\n",
    "- Practice all major aggregation and windowing techniques on real-world data\n",
    "- Learn with generic sample syntax, then apply concepts to your actual DataFrames\n",
    "- Tackle interview-style and real-world analytics questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8d325a",
   "metadata": {},
   "source": [
    "\n",
    "## Important Instructions\n",
    "- Sample syntax is for illustration only and uses generic DataFrame names (e.g., `df`, `input_df`).\n",
    "- Always use the actual DataFrame names provided in the practice questions (e.g., `customers_df`, `orders_df`).\n",
    "- Avoid using AI for code completion.\n",
    "- Play around and try out a few more for your understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70b2f89",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preparation (Run This First)\n",
    "This section downloads, loads, and cleans all datasets so you can start aggregation and windowing without running Chapter 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the data\n",
    "!wget -O customers.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/customers.csv\n",
    "!wget -O products.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/products.csv\n",
    "!wget -O orders.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/orders.csv\n",
    "!wget -O order_items.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/order_items.csv\n",
    "!wget -O employees.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/employees.csv\n",
    "!wget -O transactions.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/transactions.csv\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySpark Aggregation Practice\").getOrCreate()\n",
    "\n",
    "# Load DataFrames\n",
    "df_map = {}\n",
    "for name in [\"customers\", \"products\", \"orders\", \"order_items\", \"employees\", \"transactions\"]:\n",
    "    df_map[name] = spark.read.csv(f\"{name}.csv\", header=True, inferSchema=True)\n",
    "\n",
    "customers_df = df_map[\"customers\"]\n",
    "products_df = df_map[\"products\"]\n",
    "orders_df = df_map[\"orders\"]\n",
    "order_items_df = df_map[\"order_items\"]\n",
    "employees_df = df_map[\"employees\"]\n",
    "transactions_df = df_map[\"transactions\"]\n",
    "\n",
    "# Data cleaning (minimal, for aggregation):\n",
    "customers_df = customers_df.dropDuplicates().dropna(subset=[\"customer_id\", \"name\", \"email\"])\n",
    "products_df = products_df.dropDuplicates().dropna(subset=[\"product_id\", \"product_name\", \"price\"])\n",
    "orders_df = orders_df.dropDuplicates().dropna(subset=[\"order_id\", \"customer_id\", \"order_amount\"])\n",
    "order_items_df = order_items_df.dropDuplicates().dropna(subset=[\"order_item_id\", \"order_id\", \"product_id\"])\n",
    "employees_df = employees_df.dropDuplicates().dropna(subset=[\"employee_id\", \"name\"])\n",
    "transactions_df = transactions_df.dropDuplicates().dropna(subset=[\"transaction_id\", \"customer_id\", \"amount\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a473d56",
   "metadata": {},
   "source": [
    "\n",
    "## Table Overview\n",
    "- **customers_df**: customer_id, name, email, phone, address, registration_date, status\n",
    "- **products_df**: product_id, product_name, category, price, stock_quantity\n",
    "- **orders_df**: order_id, customer_id, order_date, order_amount, order_status, payment_method\n",
    "- **order_items_df**: order_item_id, order_id, product_id, quantity, item_total\n",
    "- **employees_df**: employee_id, name, department, hire_date, salary, manager_id\n",
    "- **transactions_df**: transaction_id, customer_id, transaction_date, amount, transaction_type, location, created_at\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc8afe",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Basic Aggregation Functions\n",
    "\n",
    "**Concept:** Use functions like `count`, `sum`, `avg`, `min`, `max` to summarize data.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "# Count rows\n",
    "df.count()\n",
    "# Sum a column\n",
    "df.agg({\"col1\": \"sum\"})\n",
    "# Average\n",
    "df.agg({\"col2\": \"avg\"})\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Find the total number of orders in `orders_df`.\n",
    "- Find the average product price in `products_df`.\n",
    "\n",
    "**Expected Output:**\n",
    "- A single number for each aggregation.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Find the minimum and maximum order amount in `orders_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e994d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dd6b66",
   "metadata": {},
   "source": [
    "\n",
    "### 2. GroupBy Aggregations\n",
    "\n",
    "**Concept:** Use `groupBy` to aggregate data by one or more columns.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "# Group by a column and aggregate\n",
    "df.groupBy(\"col1\").agg({\"col2\": \"sum\"})\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Calculate the total order amount for each customer in `orders_df`.\n",
    "- Count the number of orders for each order status in `orders_df`.\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with one row per group and the aggregated value(s).\n",
    "\n",
    "**Additional Challenge:**\n",
    "- For each product in `order_items_df`, calculate the total quantity sold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e118d6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f2fc5",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Multiple Aggregations in One Go\n",
    "\n",
    "**Concept:** Use `agg` to perform several aggregations at once.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "# Multiple aggregations\n",
    "df.groupBy(\"col1\").agg({\"col2\": \"sum\", \"col3\": \"avg\"})\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- For each product in `products_df`, get the min, max, and average price.\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with product_id and the three aggregated values.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- For each customer in `orders_df`, get the count and total order amount.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc09628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4673e76c",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Aggregation with Filtering\n",
    "\n",
    "**Concept:** Filter data before or after aggregation (like SQL WHERE and HAVING).\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "# Filter before aggregation\n",
    "filtered_df = df.filter(df.col1 > 0)\n",
    "# Filter after aggregation\n",
    "agg_df = df.groupBy(\"col1\").agg({\"col2\": \"sum\"})\n",
    "result = agg_df.filter(agg_df[\"sum(col2)\"] > 100)\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Find customers whose total order amount in `orders_df` is greater than 1000.\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with customer_id and total order amount, filtered as required.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Find products with average price above 50.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db51849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ece41",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Distinct and Set Aggregations\n",
    "\n",
    "**Concept:** Use `distinct`, `countDistinct`, `collect_set`, and `collect_list` for set-based summaries.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "# Count distinct values\n",
    "df.select(\"col1\").distinct().count()\n",
    "# Collect unique values per group\n",
    "df.groupBy(\"col2\").agg(collect_set(\"col1\"))\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- List all unique payment methods in `orders_df`.\n",
    "- For each customer, list all unique order statuses in `orders_df`.\n",
    "\n",
    "**Expected Output:**\n",
    "- A list or DataFrame of unique values.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- For each product, collect all order_ids where it was sold (as a list).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c179da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30044047",
   "metadata": {},
   "source": [
    "\n",
    "### 6. Pivot and Unpivot\n",
    "\n",
    "**Concept:** Use `pivot` to reshape data (cross-tabulation).\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "# Pivot\n",
    "df.groupBy(\"col1\").pivot(\"col2\").agg({\"col3\": \"sum\"})\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Create a pivot table of order counts by status for each payment method in `orders_df`.\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with payment methods as rows and order statuses as columns.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Pivot: For each customer, show total order amount by order status.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a00058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8babf9ac",
   "metadata": {},
   "source": [
    "\n",
    "### 7. Handling Nulls in Aggregation\n",
    "\n",
    "**Concept:** Nulls can affect aggregation results. Clean or fill as needed.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "# Fill nulls before aggregation\n",
    "df = df.fillna({\"col1\": 0})\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Calculate the average order amount in `orders_df`, ignoring nulls.\n",
    "\n",
    "**Expected Output:**\n",
    "- A single number (average order amount).\n",
    "\n",
    "**Additional Challenge:**\n",
    "- For each product, calculate total quantity sold, treating null quantities as zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854703b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4688ed01",
   "metadata": {},
   "source": [
    "\n",
    "### 8. Window Functions: Introduction\n",
    "\n",
    "**Concept:** Window functions compute values over a group of rows related to the current row.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum, row_number\n",
    "\n",
    "window_spec = Window.partitionBy(\"col1\").orderBy(\"col2\")\n",
    "df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- For each customer in `orders_df`, assign a row number to their orders by order amount (descending).\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with an extra column for row number.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- For each product, assign a rank to each sale by item_total (descending).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bfc0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef5a5b1",
   "metadata": {},
   "source": [
    "\n",
    "### 9. Row Number, Rank, Dense Rank\n",
    "\n",
    "**Concept:** Use `row_number`, `rank`, and `dense_rank` for ranking within partitions.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank\n",
    "window_spec = Window.partitionBy(\"col1\").orderBy(col(\"col2\").desc())\n",
    "df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Find the top 3 orders (by amount) for each customer in `orders_df`.\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with only the top 3 orders per customer.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- For each product, find the 2nd highest item_total in `order_items_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac5f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0624ee23",
   "metadata": {},
   "source": [
    "\n",
    "### 10. Running Totals and Moving Averages\n",
    "\n",
    "**Concept:** Use window functions for running/cumulative calculations.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "from pyspark.sql.functions import sum, avg\n",
    "window_spec = Window.partitionBy(\"col1\").orderBy(\"col2\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "df.withColumn(\"running_total\", sum(\"col3\").over(window_spec))\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- For each customer in `transactions_df`, calculate the running total of amount by transaction_date.\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with a running total column.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- For each product, calculate a 3-sale moving average of item_total in `order_items_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f901aae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e3401",
   "metadata": {},
   "source": [
    "\n",
    "### 11. Lag and Lead\n",
    "\n",
    "**Concept:** Use `lag` and `lead` to access previous/next row values in a window.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "from pyspark.sql.functions import lag, lead\n",
    "window_spec = Window.partitionBy(\"col1\").orderBy(\"col2\")\n",
    "df.withColumn(\"prev_value\", lag(\"col3\", 1).over(window_spec))\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- For each customer in `transactions_df`, calculate the difference between the current and previous transaction amount.\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with a difference column.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- For each product, show the previous and next sale amount in `order_items_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3836c443",
   "metadata": {},
   "source": [
    "\n",
    "### 12. Cumulative Aggregations\n",
    "\n",
    "**Concept:** Cumulative sum, min, max, count using window functions.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "from pyspark.sql.functions import sum\n",
    "window_spec = Window.partitionBy(\"col1\").orderBy(\"col2\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "df.withColumn(\"cumulative_sum\", sum(\"col3\").over(window_spec))\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- For each customer in `orders_df`, calculate the cumulative order amount by order_date.\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with a cumulative sum column.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- For each employee, calculate the cumulative salary paid by hire_date in `employees_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f00199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41d3c6a",
   "metadata": {},
   "source": [
    "\n",
    "### 13. Partitioning and Ordering in Windows\n",
    "\n",
    "**Concept:** Use `partitionBy` and `orderBy` to define window boundaries.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "window_spec = Window.partitionBy(\"col1\").orderBy(\"col2\")\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- For each product in `order_items_df`, rank sales by item_total within each month.\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with a rank column partitioned by product and month.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- For each customer, rank their orders by order_amount within each year in `orders_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce02fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700f02f9",
   "metadata": {},
   "source": [
    "\n",
    "### 14. Advanced Window Functions\n",
    "\n",
    "**Concept:** Use `first`, `last`, `ntile`, `percent_rank` for advanced analytics.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "from pyspark.sql.functions import first, last, ntile, percent_rank\n",
    "window_spec = Window.partitionBy(\"col1\").orderBy(\"col2\")\n",
    "df.withColumn(\"first_value\", first(\"col3\").over(window_spec))\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Assign quartiles to customers based on total spend in `transactions_df`.\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with a quartile column.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- For each product, show the first and last sale amount in `order_items_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b4ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063113f3",
   "metadata": {},
   "source": [
    "\n",
    "### 15. Aggregation with Complex Expressions\n",
    "\n",
    "**Concept:** Use conditional logic in aggregation (e.g., `when`, `case`).\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "from pyspark.sql.functions import when, sum\n",
    "# Conditional aggregation\n",
    "df.groupBy(\"col1\").agg(sum(when(df.col2 > 100, 1).otherwise(0)).alias(\"high_value_count\"))\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- For each customer in `orders_df`, count the number of high-value orders (order_amount > 200).\n",
    "\n",
    "**Expected Output:**\n",
    "- A DataFrame with customer_id and high-value order count.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- For each product, count the number of times it was sold in quantities greater than 5 in `order_items_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0598b3a7",
   "metadata": {},
   "source": [
    "\n",
    "### 16. Real-World Scenarios & Challenges\n",
    "\n",
    "- Top N per group: Top 2 products per category by total quantity sold\n",
    "- Rolling window: 7-day moving average of daily sales in `orders_df`\n",
    "- Aggregating over time windows: Total transactions per month in `transactions_df`\n",
    "- Combining groupBy and window functions: For each customer, running total of order amount by order_date\n",
    "\n",
    "**Try to solve these using the concepts above!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b0c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice here"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
