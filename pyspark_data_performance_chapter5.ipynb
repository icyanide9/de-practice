{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42ff303",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 5: Performance Optimization, Best Practices & Real-World Projects (Deep Dive)\n",
    "\n",
    "Welcome to the final and most advanced chapter! Here, you'll master PySpark performance tuning, best practices, and real-world project design. This notebook is precise, practical, and interview-ready.\n",
    "\n",
    "## What You'll Do\n",
    "- Deeply understand Spark's execution model and performance bottlenecks\n",
    "- Apply best practices for scalable, maintainable data engineering\n",
    "- Build and optimize real-world ETL pipelines\n",
    "- Solve open-ended, interview-style challenges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc8c724",
   "metadata": {},
   "source": [
    "\n",
    "## Important Instructions\n",
    "- Sample syntax is for illustration only and uses generic DataFrame names (e.g., `df`, `input_df`).\n",
    "- Always use the actual DataFrame names provided in the practice questions (e.g., `customers_df`, `orders_df`).\n",
    "- Do not copy-paste the sample code for the practice question. Try to solve it yourself using the actual DataFrame.\n",
    "- This is for your own practice, so type the commands even if the question is similar to the example.\n",
    "- Don't execute the code mentioned in syntax as it may modify the data.\n",
    "- Avoid using AI for code completion.\n",
    "- Play around and try out a few more for your understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c583d3",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preparation (Run This First)\n",
    "This section downloads, loads, and cleans all datasets so you can start performance and project work without running previous chapters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf5e743",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the data\n",
    "!wget -O customers.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/customers.csv\n",
    "!wget -O products.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/products.csv\n",
    "!wget -O orders.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/orders.csv\n",
    "!wget -O order_items.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/order_items.csv\n",
    "!wget -O employees.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/employees.csv\n",
    "!wget -O transactions.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/transactions.csv\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySpark Optimization Practice\").getOrCreate()\n",
    "\n",
    "# Load DataFrames\n",
    "df_map = {}\n",
    "for name in [\"customers\", \"products\", \"orders\", \"order_items\", \"employees\", \"transactions\"]:\n",
    "    df_map[name] = spark.read.csv(f\"{name}.csv\", header=True, inferSchema=True)\n",
    "\n",
    "customers_df = df_map[\"customers\"]\n",
    "products_df = df_map[\"products\"]\n",
    "orders_df = df_map[\"orders\"]\n",
    "order_items_df = df_map[\"order_items\"]\n",
    "employees_df = df_map[\"employees\"]\n",
    "transactions_df = df_map[\"transactions\"]\n",
    "\n",
    "# Data cleaning (minimal, for performance):\n",
    "customers_df = customers_df.dropDuplicates().dropna(subset=[\"customer_id\", \"name\", \"email\"])\n",
    "products_df = products_df.dropDuplicates().dropna(subset=[\"product_id\", \"product_name\", \"price\"])\n",
    "orders_df = orders_df.dropDuplicates().dropna(subset=[\"order_id\", \"customer_id\", \"order_amount\"])\n",
    "order_items_df = order_items_df.dropDuplicates().dropna(subset=[\"order_item_id\", \"order_id\", \"product_id\"])\n",
    "employees_df = employees_df.dropDuplicates().dropna(subset=[\"employee_id\", \"name\"])\n",
    "transactions_df = transactions_df.dropDuplicates().dropna(subset=[\"transaction_id\", \"customer_id\", \"amount\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a02f92",
   "metadata": {},
   "source": [
    "\n",
    "## Table Overview\n",
    "- **customers_df**: customer_id, name, email, phone, address, registration_date, status\n",
    "- **products_df**: product_id, product_name, category, price, stock_quantity\n",
    "- **orders_df**: order_id, customer_id, order_date, order_amount, order_status, payment_method\n",
    "- **order_items_df**: order_item_id, order_id, product_id, quantity, item_total\n",
    "- **employees_df**: employee_id, name, department, hire_date, salary, manager_id\n",
    "- **transactions_df**: transaction_id, customer_id, transaction_date, amount, transaction_type, location, created_at\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ef2d8",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Spark Execution Model: Jobs, Stages, Tasks\n",
    "\n",
    "**Concept:**\n",
    "- Spark breaks a job into stages (wide transformations like groupBy/join) and then into tasks (one per partition).\n",
    "- Understanding this helps you optimize shuffles and partitioning.\n",
    "- Use `explain()` to see the physical plan and identify shuffles.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "df.explain(True)\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Use `explain()` on a DataFrame after a join or groupBy to see the execution plan.\n",
    "\n",
    "**Expected Output:**\n",
    "- The physical plan showing stages and shuffles.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Identify a stage in the plan that causes a shuffle and suggest an optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a8ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56baed9b",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Wide vs. Narrow Transformations\n",
    "\n",
    "**Concept:**\n",
    "- Narrow: Each partition of parent RDD is used by at most one partition of the child (e.g., map, filter).\n",
    "- Wide: Multiple parent partitions are needed (e.g., groupBy, join), causing shuffles.\n",
    "- Minimize wide transformations for better performance.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "# Narrow transformation\n",
    "df2 = df1.filter(df1.col1 > 0)\n",
    "# Wide transformation\n",
    "df3 = df2.groupBy(\"col2\").agg({\"col3\": \"sum\"})\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Identify which transformations in your pipeline are wide and which are narrow.\n",
    "\n",
    "**Expected Output:**\n",
    "- List of transformations classified as wide or narrow.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Refactor a pipeline to reduce the number of wide transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d30e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687d712b",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Partitioning Strategies and Data Skew\n",
    "\n",
    "**Concept:**\n",
    "- Use `repartition()` or `coalesce()` to control partition count and distribution.\n",
    "- Data skew occurs when some partitions have much more data than others, causing slow tasks.\n",
    "- Mitigate skew by salting keys or increasing partitions.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "# Repartition by key\n",
    "df = df.repartition(10, \"col1\")\n",
    "# Coalesce to reduce partitions\n",
    "df = df.coalesce(2)\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Repartition a large DataFrame by a high-cardinality key and observe performance.\n",
    "- Simulate skew and fix it by salting the join key.\n",
    "\n",
    "**Expected Output:**\n",
    "- Improved partition distribution and faster job completion.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Write code to detect skewed partitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a54c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2ac42c",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Predicate Pushdown & Column Pruning\n",
    "\n",
    "**Concept:**\n",
    "- Apply `filter()` and `select()` as early as possible to reduce data read and shuffle.\n",
    "- Predicate pushdown lets Spark skip reading irrelevant data blocks.\n",
    "- Column pruning reads only necessary columns.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "df = df.filter(df.col1 > 0).select(\"col1\", \"col2\")\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Compare execution plans with and without early filtering using `explain()`.\n",
    "\n",
    "**Expected Output:**\n",
    "- More efficient physical plan with early filtering.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Measure and compare execution time for both approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c817de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db243fd",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Broadcast Joins\n",
    "\n",
    "**Concept:**\n",
    "- Use `broadcast()` to join a small DataFrame to a large one, avoiding shuffles.\n",
    "- Only use when the small DataFrame fits in memory.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "joined_df = df1.join(broadcast(df2), df1.key == df2.key)\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Broadcast join `products_df` to `order_items_df` and compare execution plans.\n",
    "\n",
    "**Expected Output:**\n",
    "- Faster join and no shuffle in the plan.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Try to broadcast a large DataFrame and observe the warning/error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c7a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fd3a42",
   "metadata": {},
   "source": [
    "\n",
    "### 6. Caching and Persisting DataFrames\n",
    "\n",
    "**Concept:**\n",
    "- Cache DataFrames only when reused multiple times.\n",
    "- `cache()` stores in memory; `persist()` can use memory and disk.\n",
    "- Unpersist when done to free resources.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "df.cache()\n",
    "df.persist()\n",
    "df.unpersist()\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Cache a DataFrame, run actions, and observe performance improvement.\n",
    "\n",
    "**Expected Output:**\n",
    "- Faster repeated actions on the cached DataFrame.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Measure memory usage before and after caching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d5be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373a63ee",
   "metadata": {},
   "source": [
    "\n",
    "### 7. File Formats and Compression\n",
    "\n",
    "**Concept:**\n",
    "- Use columnar formats like Parquet or ORC for analytics; they support predicate pushdown and compression.\n",
    "- CSV is row-based and less efficient for large-scale analytics.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "df.write.parquet(\"output.parquet\")\n",
    "df.write.csv(\"output.csv\")\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Save a DataFrame as Parquet and as CSV, then compare file size and read performance.\n",
    "\n",
    "**Expected Output:**\n",
    "- Smaller file size and faster reads with Parquet.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Try different compression codecs (e.g., snappy, gzip) and compare results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1703448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56b915d",
   "metadata": {},
   "source": [
    "\n",
    "### 8. Monitoring and Debugging with Spark UI\n",
    "\n",
    "**Concept:**\n",
    "- Use Spark UI to monitor jobs, stages, and tasks.\n",
    "- Look for long-running or failed stages, skewed tasks, and memory issues.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "# No code—access Spark UI via the cluster or notebook environment.\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Run a job, open Spark UI, and identify bottlenecks.\n",
    "\n",
    "**Expected Output:**\n",
    "- List of slow stages and possible causes.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Suggest optimizations based on Spark UI findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0aaf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36d955f",
   "metadata": {},
   "source": [
    "\n",
    "### 9. Best Practices: Schema Management, Modular Code, Logging\n",
    "\n",
    "**Concept:**\n",
    "- Always define schemas for production jobs to avoid inference errors.\n",
    "- Write modular, reusable code with functions and clear variable names.\n",
    "- Use logging and error handling for production pipelines.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "schema = StructType([StructField(\"col1\", StringType(), True)])\n",
    "df = spark.read.schema(schema).csv(\"file.csv\")\n",
    "\n",
    "def clean_data(df):\n",
    "    return df.dropDuplicates().dropna()\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.info(\"Pipeline started\")\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Refactor a notebook cell into a function and add logging.\n",
    "\n",
    "**Expected Output:**\n",
    "- Cleaner, more maintainable code with logs.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Add error handling to your function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c65065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a97fc4",
   "metadata": {},
   "source": [
    "\n",
    "### 10. Data Quality Checks and Validation\n",
    "\n",
    "**Concept:**\n",
    "- Check for nulls, duplicates, and out-of-range values.\n",
    "- Write assertions and validation functions to ensure data quality.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "assert df.filter(df.col1.isNull()).count() == 0, \"Nulls found in col1\"\n",
    "invalid_rows = df.filter(df.col2 < 0)\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Write a function to validate that `order_amount` in `orders_df` is always positive.\n",
    "\n",
    "**Expected Output:**\n",
    "- Assertion passes or fails with a clear message.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Add automated data quality checks to your ETL pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f72d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4e4ca3",
   "metadata": {},
   "source": [
    "\n",
    "### 11. Real-World ETL Pipeline: End-to-End Example\n",
    "\n",
    "**Concept:**\n",
    "- Build a pipeline: load, clean, transform, aggregate, and save data with all optimizations and best practices.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "raw_df = spark.read.csv(\"raw.csv\", header=True)\n",
    "clean_df = raw_df.dropDuplicates().dropna()\n",
    "agged_df = clean_df.groupBy(\"col1\").agg({\"col2\": \"sum\"})\n",
    "agged_df.write.parquet(\"output.parquet\")\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Build an ETL pipeline: load raw orders, clean data, aggregate total sales per customer, and save as Parquet.\n",
    "\n",
    "**Expected Output:**\n",
    "- End-to-end pipeline code and results.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Add data quality checks and performance optimizations to your pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed48018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e93223",
   "metadata": {},
   "source": [
    "\n",
    "### 12. Interview-Style Challenges & Capstone Exercises\n",
    "\n",
    "**Concept:**\n",
    "- Tackle open-ended, multi-step data engineering problems.\n",
    "- Optimize, debug, and design scalable pipelines.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "# No single syntax—combine all skills learned so far.\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Given a slow-running job, identify and fix the bottleneck.\n",
    "- Debug and fix a broken ETL pipeline (e.g., missing data, schema mismatch).\n",
    "- Design a scalable pipeline for a business scenario (e.g., daily sales reporting for millions of records).\n",
    "\n",
    "**Expected Output:**\n",
    "- Optimized, working code and clear explanations.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Document your solution and explain your design choices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2178f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
