{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a6a4e4",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 5: Performance Optimization, Best Practices & Real-World Projects\n",
    "\n",
    "Welcome to the final chapter! Here, you'll learn how to optimize PySpark jobs, follow best practices, and tackle real-world data engineering projects. This chapter is standalone: all data is freshly loaded and cleaned before you begin practicing.\n",
    "\n",
    "## What You'll Do\n",
    "- Master performance tuning and best practices in PySpark\n",
    "- Apply your skills to real-world, end-to-end data engineering scenarios\n",
    "- Prepare for interviews and production work with capstone challenges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668dff12",
   "metadata": {},
   "source": [
    "\n",
    "## Important Instructions\n",
    "- Sample syntax is for illustration only and uses generic DataFrame names (e.g., `df`, `input_df`).\n",
    "- Always use the actual DataFrame names provided in the practice questions (e.g., `customers_df`, `orders_df`).\n",
    "- Do not copy-paste the sample code for the practice question. Try to solve it yourself using the actual DataFrame.\n",
    "- This is for your own practice, so type the commands even if the question is similar to the example.\n",
    "- Don't execute the code mentioned in syntax as it may modify the data.\n",
    "- Avoid using AI for code completion.\n",
    "- Play around and try out a few more for your understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142ee75",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preparation (Run This First)\n",
    "This section downloads, loads, and cleans all datasets so you can start performance and project work without running previous chapters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68945427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the data\n",
    "!wget -O customers.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/customers.csv\n",
    "!wget -O products.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/products.csv\n",
    "!wget -O orders.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/orders.csv\n",
    "!wget -O order_items.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/order_items.csv\n",
    "!wget -O employees.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/employees.csv\n",
    "!wget -O transactions.csv https://raw.githubusercontent.com/icyanide9/de-practice/refs/heads/main/transactions.csv\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySpark Optimization Practice\").getOrCreate()\n",
    "\n",
    "# Load DataFrames\n",
    "df_map = {}\n",
    "for name in [\"customers\", \"products\", \"orders\", \"order_items\", \"employees\", \"transactions\"]:\n",
    "    df_map[name] = spark.read.csv(f\"{name}.csv\", header=True, inferSchema=True)\n",
    "\n",
    "customers_df = df_map[\"customers\"]\n",
    "products_df = df_map[\"products\"]\n",
    "orders_df = df_map[\"orders\"]\n",
    "order_items_df = df_map[\"order_items\"]\n",
    "employees_df = df_map[\"employees\"]\n",
    "transactions_df = df_map[\"transactions\"]\n",
    "\n",
    "# Data cleaning (minimal, for performance):\n",
    "customers_df = customers_df.dropDuplicates().dropna(subset=[\"customer_id\", \"name\", \"email\"])\n",
    "products_df = products_df.dropDuplicates().dropna(subset=[\"product_id\", \"product_name\", \"price\"])\n",
    "orders_df = orders_df.dropDuplicates().dropna(subset=[\"order_id\", \"customer_id\", \"order_amount\"])\n",
    "order_items_df = order_items_df.dropDuplicates().dropna(subset=[\"order_item_id\", \"order_id\", \"product_id\"])\n",
    "employees_df = employees_df.dropDuplicates().dropna(subset=[\"employee_id\", \"name\"])\n",
    "transactions_df = transactions_df.dropDuplicates().dropna(subset=[\"transaction_id\", \"customer_id\", \"amount\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f062e1f9",
   "metadata": {},
   "source": [
    "\n",
    "## Table Overview\n",
    "- **customers_df**: customer_id, name, email, phone, address, registration_date, status\n",
    "- **products_df**: product_id, product_name, category, price, stock_quantity\n",
    "- **orders_df**: order_id, customer_id, order_date, order_amount, order_status, payment_method\n",
    "- **order_items_df**: order_item_id, order_id, product_id, quantity, item_total\n",
    "- **employees_df**: employee_id, name, department, hire_date, salary, manager_id\n",
    "- **transactions_df**: transaction_id, customer_id, transaction_date, amount, transaction_type, location, created_at\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5060ab3b",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Performance Optimization in PySpark\n",
    "\n",
    "**Concept:**\n",
    "- Spark executes jobs as a DAG of stages and tasks. Understanding this helps you optimize your code.\n",
    "- Avoid wide transformations (like groupBy, join) when possible, as they cause shuffles.\n",
    "- Use partitioning to distribute data evenly and avoid data skew.\n",
    "- Use predicate pushdown and column pruning by filtering and selecting only needed columns early.\n",
    "- Use `broadcast()` joins for small tables to avoid shuffles.\n",
    "- Cache or persist DataFrames only when reused multiple times.\n",
    "- Monitor jobs using the Spark UI to identify bottlenecks.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "# Broadcast join\n",
    "joined_df = df1.join(broadcast(df2), df1.key == df2.key)\n",
    "# Cache DataFrame\n",
    "df.cache()\n",
    "# Filter early for predicate pushdown\n",
    "df = df.filter(df.col1 > 0).select(\"col1\", \"col2\")\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Identify a wide transformation in your pipeline and suggest an optimization.\n",
    "- Broadcast join `products_df` to `order_items_df` if products is small.\n",
    "\n",
    "**Expected Output:**\n",
    "- Optimized code and explanation.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Use the Spark UI to find a slow stage and explain how you would optimize it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982844c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0411a",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Best Practices for PySpark Data Engineering\n",
    "\n",
    "**Concept:**\n",
    "- Use columnar formats like Parquet or ORC for efficient storage and processing.\n",
    "- Define schemas explicitly for better performance and data quality.\n",
    "- Handle skewed data by salting keys or increasing partitions.\n",
    "- Write modular, reusable code with functions and clear variable names.\n",
    "- Use logging and error handling for production pipelines.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "# Define schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "schema = StructType([StructField(\"col1\", StringType(), True)])\n",
    "df = spark.read.schema(schema).csv(\"file.csv\")\n",
    "# Example of modular code\n",
    "def clean_data(df):\n",
    "    return df.dropDuplicates().dropna()\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Refactor a PySpark script to use explicit schema and modular functions.\n",
    "- Save a DataFrame as Parquet and explain why itâ€™s preferred over CSV.\n",
    "\n",
    "**Expected Output:**\n",
    "- Cleaner, more efficient code and explanation.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Handle a skewed join by salting the join key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba3e43",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Real-World Scenarios & Mini-Projects\n",
    "\n",
    "**Concept:**\n",
    "- Combine all your skills to build end-to-end ETL pipelines and analytics solutions.\n",
    "- Use UDFs, SQL, joins, aggregations, and optimizations together.\n",
    "- Monitor data quality and performance throughout the pipeline.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "# Example ETL pipeline\n",
    "raw_df = spark.read.csv(\"raw.csv\", header=True)\n",
    "clean_df = raw_df.dropDuplicates().dropna()\n",
    "agged_df = clean_df.groupBy(\"col1\").agg({\"col2\": \"sum\"})\n",
    "agged_df.write.parquet(\"output.parquet\")\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Build an ETL pipeline: load raw orders, clean data, aggregate total sales per customer, and save as Parquet.\n",
    "- Create a customer analytics dashboard: top customers, monthly sales, churn candidates.\n",
    "\n",
    "**Expected Output:**\n",
    "- End-to-end pipeline code and results.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Add data quality checks and performance optimizations to your pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f0448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e42ae",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Interview-Style Challenges & Capstone Exercises\n",
    "\n",
    "**Concept:**\n",
    "- Tackle open-ended, multi-step data engineering problems.\n",
    "- Optimize, debug, and design scalable pipelines.\n",
    "\n",
    "**Sample Syntax (Generic):**\n",
    "```python\n",
    "# No single syntaxâ€”combine all skills learned so far.\n",
    "```\n",
    "\n",
    "**Practice:**\n",
    "- Given a slow-running job, identify and fix the bottleneck.\n",
    "- Debug and fix a broken ETL pipeline (e.g., missing data, schema mismatch).\n",
    "- Design a scalable pipeline for a business scenario (e.g., daily sales reporting for millions of records).\n",
    "\n",
    "**Expected Output:**\n",
    "- Optimized, working code and clear explanations.\n",
    "\n",
    "**Additional Challenge:**\n",
    "- Document your solution and explain your design choices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf07ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice here"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
